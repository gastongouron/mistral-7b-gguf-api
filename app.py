#!/usr/bin/env python3
"""
API FastAPI pour servir le mod√®le Mistral 7B GGUF avec llama-cpp-python
Avec authentification Bearer et format de r√©ponse Claude-like
"""
import os
import time
import uuid
import json
import re
import logging
from fastapi import FastAPI, HTTPException, Depends, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
from llama_cpp import Llama

# Configuration
MODEL_PATH = "/app/models/mistral-7b-instruct-v0.1.Q4_K_M.gguf"
MODEL_URL = "https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf"
API_TOKEN = os.getenv("API_TOKEN", "supersecret")  # Peut √™tre d√©fini via variable d'environnement

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# S√©curit√©
security = HTTPBearer()

# Mod√®les Pydantic
class Message(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str = "mistral-7b-gguf"
    messages: List[Message]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512
    top_p: Optional[float] = 0.95
    stream: Optional[bool] = False
    stop: Optional[List[str]] = None
    response_format: Optional[Dict[str, str]] = None  # Pour forcer JSON

class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class Choice(BaseModel):
    index: int
    message: Message
    finish_reason: str

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Choice]
    usage: Usage

# Initialisation de l'application
app = FastAPI(
    title="Mistral 7B GGUF API",
    version="1.0.0",
    description="API FastAPI pour Mistral 7B avec llama-cpp-python et authentification"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Variable globale pour le mod√®le
llm = None

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """V√©rifier le token Bearer"""
    if credentials.credentials != API_TOKEN:
        raise HTTPException(
            status_code=401,
            detail="Invalid authentication token",
            headers={"WWW-Authenticate": "Bearer"},
        )
    return credentials.credentials

def download_model_if_needed():
    """T√©l√©charger le mod√®le s'il n'existe pas"""
    if not os.path.exists(MODEL_PATH):
        print(f"Mod√®le non trouv√©. T√©l√©chargement depuis {MODEL_URL}...")
        os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
        
        import httpx
        with httpx.stream("GET", MODEL_URL) as response:
            response.raise_for_status()
            with open(MODEL_PATH, "wb") as f:
                for chunk in response.iter_bytes():
                    f.write(chunk)
        print("T√©l√©chargement termin√©!")

def load_model():
    """Charger le mod√®le GGUF"""
    global llm
    
    # V√©rifier/t√©l√©charger le mod√®le
    download_model_if_needed()
    
    print(f"Chargement du mod√®le depuis {MODEL_PATH}...")
    
    # Param√®tres pour llama-cpp-python
    llm = Llama(
        model_path=MODEL_PATH,
        n_ctx=4096,  # Taille du contexte
        n_threads=8,  # Nombre de threads CPU
        n_gpu_layers=35,  # Nombre de couches sur GPU
        verbose=True
    )
    
    print("Mod√®le charg√© avec succ√®s!")

def format_messages_mistral(messages: List[Message]) -> str:
    """Formater les messages pour Mistral Instruct"""
    formatted = ""
    
    for i, message in enumerate(messages):
        if message.role == "system":
            formatted += f"[INST] {message.content}\n"
        elif message.role == "user":
            if i == 0 or messages[i-1].role != "system":
                formatted += f"[INST] {message.content} [/INST]"
            else:
                formatted += f"{message.content} [/INST]"
        elif message.role == "assistant":
            formatted += f" {message.content}</s> "
    
    return formatted.strip()

def clean_and_parse_json(text: str) -> Optional[Dict]:
    """Nettoyer et parser du JSON potentiellement mal format√©"""
    # Am√©lioration du nettoyage JSON
    # Enlever les timestamps et pr√©fixes  
    text = re.sub(r'^\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}\.\d+ \[.*?\] => ', '', text.strip())
    text = re.sub(r'^.*?Extracted content:\s*', '', text, flags=re.IGNORECASE)
    text = re.sub(r'^.*?:\s*(?=\{)', '', text)  # Enlever tout avant le premier {
    
    # Extraire uniquement le JSON
    json_start = text.find('{')
    json_end = text.rfind('}')
    
    if json_start != -1 and json_end != -1 and json_end > json_start:
        json_text = text[json_start:json_end+1]
    else:
        # Pas de JSON trouv√©
        return None
    
    # Corriger les probl√®mes courants
    # Remplacer les underscores √©chapp√©s
    json_text = json_text.replace(r'\_', '_')
    
    # Essayer de parser
    try:
        return json.loads(json_text)
    except json.JSONDecodeError:
        # Essayer de corriger les guillemets simples
        json_text = json_text.replace("'", '"')
        try:
            return json.loads(json_text)
        except:
            return None

def ensure_json_response(text: str, request_format: Optional[Dict] = None) -> str:
    """S'assurer que la r√©ponse est du JSON valide si demand√©"""
    if request_format and request_format.get("type") == "json_object":
        # Essayer de parser et nettoyer le JSON
        parsed = clean_and_parse_json(text)
        if parsed:
            return json.dumps(parsed, ensure_ascii=False)
        else:
            # Si on ne peut pas parser, cr√©er une structure JSON basique
            return json.dumps({
                "response": text,
                "error": "Could not parse as valid JSON"
            }, ensure_ascii=False)
    return text

@app.on_event("startup")
async def startup_event():
    """Charger le mod√®le au d√©marrage"""
    load_model()

@app.get("/")
async def root():
    """Point d'entr√©e de l'API - pas d'auth n√©cessaire"""
    return {
        "message": "Mistral 7B GGUF API",
        "status": "running",
        "model": "mistral-7b-instruct-v0.1.Q4_K_M.gguf",
        "endpoints": {
            "/v1/chat/completions": "POST - Chat completions endpoint (requires Bearer token)",
            "/v1/models": "GET - List available models",
            "/health": "GET - Health check"
        }
    }

@app.get("/health")
async def health_check():
    """V√©rifier l'√©tat de l'API - pas d'auth n√©cessaire"""
    return {
        "status": "healthy",
        "model_loaded": llm is not None,
        "model_path": MODEL_PATH,
        "model_exists": os.path.exists(MODEL_PATH)
    }

@app.get("/v1/models", dependencies=[Depends(verify_token)])
async def list_models():
    """Lister les mod√®les disponibles"""
    return {
        "object": "list",
        "data": [
            {
                "id": "mistral-7b-gguf",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "TheBloke",
                "permission": [],
                "root": "mistral-7b-gguf",
                "parent": None
            }
        ]
    }

@app.post("/v1/chat/completions", response_model=ChatCompletionResponse, dependencies=[Depends(verify_token)])
async def chat_completions(request: ChatCompletionRequest):
    """Endpoint compatible OpenAI pour les compl√©tions de chat"""
    if llm is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    # ‚è±Ô∏è D√âBUT MESURE PERFORMANCE
    start_time = time.time()
    
    try:
        # Formater les messages
        prompt = format_messages_mistral(request.messages)
        
        # Si on demande du JSON, ajouter des instructions FORTES
        if request.response_format and request.response_format.get("type") == "json_object":
            prompt += "\n[INST] CRITICAL: R√©ponds UNIQUEMENT avec un objet JSON valide. PAS de texte avant ou apr√®s. PAS d'explication. SEULEMENT le JSON entre { et }. [/INST]"
        
        # ‚è±Ô∏è D√âBUT G√âN√âRATION
        generation_start = time.time()
        
        # G√©n√©rer la r√©ponse
        response = llm(
            prompt,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            top_p=request.top_p,
            stop=request.stop or ["</s>", "[INST]"],
            echo=False
        )
        
        # ‚è±Ô∏è FIN G√âN√âRATION
        generation_time = (time.time() - generation_start) * 1000  # en ms
        
        # Extraire le texte g√©n√©r√©
        generated_text = response['choices'][0]['text'].strip()
        
        # S'assurer que c'est du JSON valide si demand√©
        generated_text = ensure_json_response(generated_text, request.response_format)
        
        # ‚è±Ô∏è TEMPS TOTAL
        total_time = (time.time() - start_time) * 1000  # en ms
        
        # üìä LOG PERFORMANCE
        logging.info(f"[PERF] Total: {total_time:.0f}ms | Generation: {generation_time:.0f}ms | Tokens: {len(response['usage']['completion_tokens'])} | Format: {request.response_format}")
        
        # Cr√©er la r√©ponse au format OpenAI/Claude
        chat_response = ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex[:8]}",
            created=int(time.time()),
            model=request.model,
            choices=[
                Choice(
                    index=0,
                    message=Message(role="assistant", content=generated_text),
                    finish_reason=response['choices'][0]['finish_reason']
                )
            ],
            usage=Usage(
                prompt_tokens=response['usage']['prompt_tokens'],
                completion_tokens=response['usage']['completion_tokens'],
                total_tokens=response['usage']['total_tokens']
            )
        )
        
        return chat_response
        
    except Exception as e:
        print(f"Erreur lors de la g√©n√©ration: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/v1/chat/completions")
async def chat_completions_info():
    """Information sur l'endpoint de chat"""
    return {
        "error": "This endpoint only supports POST requests",
        "hint": "Send a POST request with a JSON body containing 'messages' array and Bearer token in header"
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)